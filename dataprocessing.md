# Data Processing Implementation
## Initial Planning
### Process
- Create an image repository in GitHub.
- When there are changes in the Image Repository, only the modified files will be uploaded to the S3 Bucket, handled through GitHub Action.
- Upon an upload event to the S3 Bucket, the files will be stored under the `images/` prefix.
- Raw images will be stored in the `raw` directory.
- When stored, an event trigger will occur.
- The event trigger will execute a Lambda function, and the Lambda function will generate three additional resolution-specific files, which will then be uploaded to the S3 Bucket and recorded in DynamoDB.
- The resolution-specific files will be stored in the `sg`, `md`, and `lg` directories with the same filename.
- Uploaded files will have a unique filename in the format `YYMMDDHHMISS-8digit`, where the 8-digit value is a hash generated by a hash function.

### Github Action
- Executed when a Push event occurs on the main branch.
- Prevents Push for non-image files by adding an event trigger.
- If no issues are detected, the action executes a command to upload only the modified files to the S3 Bucket.
- Example code
```Yaml
name: Restrict Non-Image Files on Push

on:
  push:
    branches:
      - main

jobs:
  check-files:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Check if only image files were pushed
      run: |
        # Define the file extensions considered as images
        IMAGE_FILES=$(git diff --name-only HEAD^ HEAD | grep -E '\.(png|jpg|jpeg|gif|bmp|svg)$')

        # If there are no image files, fail the job
        if [ -z "$IMAGE_FILES" ]; then
          echo "No image files were pushed. Failing the job."
          exit 1
        fi

    - name: Success Message
      run: echo "Only image files were pushed, proceeding with the workflow."
```

### DynamoDB
- As it is NoSQL, no predefined schema is required. The following defines the key values
- `ImageId`: Managed as the primary key in the format `{S3Filename}_{FileType}`.
- `RegisteredFile`: The location and filename as registered in the GitHub repository.
- `UploadDate`: Date of registration.
- `StoredKey`: Key where the file is stored in the S3 Bucket.
- `ImageType`: One of `raw/sg/md/lg`.
- `ImageSize`: The size of the image.
- `Resolution`: The image resolution.
- `Status`: One of `failed/pending/completed`.
- `ErrorMessage`: A field added only in case of an error, recording the error message.

### Lambda Function
- Triggered by an S3 event.
- Inserts the original image information into DynamoDB.
- Converts the image into different resolutions and stores them in the S3 Bucket.
- Inserts the information of the original and resolution-specific converted images into DynamoDB.
- If conversion fails:
  - The Lambda function will attempt to retry twice.
  - If still unsuccessful, it will log the failure and error message in DynamoDB.
  - Issues with the original image will already have been validated by the GitHub Action, so no additional checks will be performed at this stage.

## Implementation - Level 100
### Objectives
- Connect github repository to API gateway
- Implement when request PUT and DELETE method, insert and delete the file in the S3 image bucket.

### OIDC(OpenID Connect) 
#### References
- [Configuring OpenID Connect in Amazon Web Services](https://docs.github.com/en/actions/security-for-github-actions/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services)
- [About security hardening with OpenID Connect](https://docs.github.com/en/actions/security-for-github-actions/security-hardening-your-deployments/about-security-hardening-with-openid-connect)
- [Create an OpenID Connect (OIDC) identity provider in IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html)

#### Overview
- There are several ways to access AWS resources from GitHub Actions, but one of the most common is using OIDC. (For more details, see the References section above)
- To configure OIDC, settings are required on both GitHub and AWS.

#### AWS IAM - Identity Provider
- Since the credentials must be handled externally (i.e., from GitHub), create an Identity Provider for this purpose.
- Configure Provider: `OpenID Connect`
- Provider URL: `https://token.actions.githubusercontent.com`
- Audience: `sts.amazonaws.com`
- After creating the identify provider, Click assign role.

### AWS IAM - Role
- Name: `apigateway-role`
- Trusted entity type: `Web Identity` (default)
- Select the audience
- Github Organization: organization or your account name
- Github Repository: a specific repository
- Github Branch: a specific repository (e.g. `main`)
- Add Permissions: `AmazonAPIGatewayInvokeFullAccess`
![image](https://github.com/user-attachments/assets/71c3a70c-7c1f-4687-8a52-03d5f1468d79)

### Github Repository Settings
- Go `Secrets and variables` - `Actions`
- Secrets
  - `API_KEY`: your API Gateway's API key
- Variables
  - `AWS_IAM_ID`: your IAM ID
  - `AWS_INVOKE_ROLE`: The IAM Role you recently created
  - `AWS_RESOURCE_ID`: API Gateway Resource ID
  - `AWS_RES_API_ID`: API Gateway ID

### Github Action Setting
- Click `New Workflow` - `set up a workflow yourself`
- See the workflow: [Click](https://github.com/realworldismine/snail-images/blob/main/.github/workflows/api.yml)
- Ensure it only runs on push actions to the main branch.
- `actions/checkout@v4`
  - You must set `fetch-depth: 2`. Otherwise, issues will arise when comparing the previous commit’s HEAD later on.
- `aws-actions/configure-aws-credentials@v4`
  - Enter the ARN and region of the IAM Role you recently created.
  - It’s recommended to use Action Variables for the ARN instead of entering it directly.

### Github Action Shell Script
- The basic operation is as follows:
  - It compares the previous and current commits to check for changes, then processes only image files.
  - If a file in the commit is added, modified, or copied, it performs a PUT request.
  - If a file in the commit is deleted, it performs a DELETE request.
  - If a file in the commit is renamed, it first performs a DELETE request for the old name, then a PUT request for the new name.
  - For renames, similarity detection is used instead of a simple R, so it may appear as R100, R95, etc. The script should be set to run when the change type starts with R.
  - Since different content types are needed for different image extensions, ensure the correct content type is sent in the header during PUT requests.
  - PUT and DELETE requests use an API Key, so make sure the Action Secret contains the API Key value.
- The Operation is as follows.
![image](https://github.com/user-attachments/assets/7770b568-cc3d-48ce-a0c7-15676220e06c)
![image](https://github.com/user-attachments/assets/b05e9c6a-1bdd-46ff-8682-c2383464f20e)

### To-do Next
- In this level, we implemented the basic credentials and data transmission between GitHub Actions and AWS.
- In the next level, we will implement file name transformation and metadata storage and management using DynamoDB and Lambda functions.

## Implementation - Level 200
### Objectives
- Change API Gateway's target from S3 bucket to lambda function.
- Create a DynamoDB table.
- Implement the lambda function for insert to dynamodb table and add the file to the S3 image bucket.

### References
- [DynamoDB - Lambda Setting](https://velog.io/@nari120/DynamoDB-Lambda-%EC%98%88%EC%A0%9C)
- [Lambda - PIL error resolution](https://velog.io/@silver_bell/AWS-Lambda-PIL-%EC%98%A4%EB%A5%98-%ED%95%B4%EA%B2%B0-python)
- [Python 3.11 Package ARN](https://api.klayers.cloud/api/v2/p3.11/layers/latest/ap-northeast-2/html)

### DynamoDB
- Create a table
  - Table name: `image-metadata`
  - Partition key: `ImageType`
  - Sort key: `ImageId`
  - Default Setting

### Create a lambda insertion function
- Name: `ImageInsertProcess`
- Code
```Python
# Combined a source code and a pseudo code
...

s3_client = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

DYNAMODB_TABLE_NAME = os.getenv('DYNAMODB_TABLE_NAME')
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')

def lambda_handler(event, context):
    try:
        # image decoding
        # get an original file name
        # check an extension whether a file is an image or not
        # make a hash value
        # make a hashed file name

        # check current original file
        # if exists uses current id, otherwise then uses new id
        # put a raw file to S3 object with the hashed file name

        # declare the dynamodb table
        # make a image id using imagetype/originalfile/hashedfile
        # insert an item with the raw file's metadata

        # open an image
        # declare image resolutions

        for image_type, size in resolutions.items():
            # make a resized image
            # save a resized image using BytesIO buffer
            # put a resized file to S3 object with the hashed file name
            # insert an item with the resized file's metadata

        # return 200 succcess code

    except UnidentifiedImageError:
        # return 400 invalid image error code
    except Exception as e:
        # return 500 error code
```
### IAM 
#### Policy
- Add a policy: `AmazonDynamoDBBasicAccess`
  - Service: `DynamoDB`
  - Action: `dynamodb:PutItem`, `dynamodb:DeleteItem`, `dynamodb:GetItem`, `dynamodb:UpdateItem`, `dynamodb:Query`, `dynamodb:Scan`
- Add a policy: `AmazonS3BasicAccess`
  - Service: `S3`
  - Action: `s3:PutObject`, `s3:GetObject`, `s3:DeleteObject`

#### Role
- Modify a role: `apigateway-role`
  - Add a inline policy: `AmazonLambdaInvokeAccess`
    - Action: `lambda:InvokeFunction`

- Modify a role: `ImageInsertProcess-role-xxxx`
  - When creates a lambda function, also creates a role about the function.
  - The role contains `logs:CreateLogStream`, `logs:CreateLogGroup`, `logs:PutLogEvents` automatically.
  - Add policies: `AmazonDynamoDBBasicAccess`, `AmazonS3BasicAccess`

### Modify configurations of the lambda function(`ImageInsertProcess`)
- General configuration
  - Change the memory size: 1024MB
  - Change the ephemeral storage: 512MB
- Environment variables
  - `DYNAMODB_TABLE_NAME`: the table of DynamoDB(`image-metadata`)
  - `S3_BUCKET_NAME`: the image bucket of S3(`snail-images-dev`)
- Add a layer
  - refer to Python 3.11 Package ARN, find `PILLOW`, and then input the ARN, `arn:aws:lambda:ap-northeast-2:770693421928:layer:Klayers-p311-Pillow:5`
- If the configuration is comleted, click `Deploy`.

### Change API Gateway settings
- Change `DELETE` and `PUT` method API
- `Integration Request`
  - Change integration type to `Lambda function`.
  - Enable `Lambda proxy integration`.
    - If lambda proxy integration is disabled, the lambda function could not access the the image file.
    - Refer to [Lambda proxy integrations in API Gateway](https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html)
  - Input Lambda function's ARN
- Deploy the API.

### Change Github Action Shell Script
- Add to `-H "filename: $first_file" \` to the CURL scripts of a PUT method.
- Example
```Shell
curl -X PUT "$API_ENDPOINT/$second_file" \
  -H "Content-Type: $content_type" \
  -H "x-api-key: ${{ secrets.API_KEY }}" \
  -H "filename: $second_file" \
  --data-binary "@$second_file"
```
- In a lambda function, a code of get an original filename is below.
```Python
original_filename = event['headers']['filename']
```
- However, a HTTP header doesn't contain an attached file's name, so a custom header name of the file's name is needed.
- The custom header name is `filename`.
- In github action, it also needs  an additional header when CURL request is executed.

## Implementation - Level 250
### Objectives
- Implement the lambda function for deleting the image file metadata to dynamodb table and delete the image file to the S3 image bucket.

### Change DynamoDB Keys
- Add an index
  - In deletion, a query execution is contained.
  - For where condition for a query, it uses an index.
  - Index Key
    - Partition Key: `RegisteredFile`
    - Sort Key: `Status`

### Image Processing in Lambda, DynamoDB, Se3 Bucket
- Suppose you have the following code.
```Python
image_data = base64.b64decode(event['body'])

hash_object = hashlib.sha256(image_data)
hashed_filename = hash_object.hexdigest()[:20] + f'.{file_extension}'
```
- This code is a simple algorithm that converts to a random value using a hashing algorithm.
- Upon closer inspection, it converts the image data into a hash value.
- The important point here is that the same image will produce the same hashed value.
- In GitHub Action, if a file name is changed:
  - In this case, the old file is deleted, and the new file is uploaded.
  - The actual operation on the S3 bucket is to delete and then recreate a file with the same name.
  - Since the original filenames are different in DynamoDB, a delete marker is generated for the previous image ID, and a new image ID is inserted.
- If a new file is added in GitHub Action, but another file with the same image already exists:
  - Since the images are identical, the hash values will also be identical.
  - Therefore, in the S3 bucket, the file is uploaded only with a different registration date.
  - However, in DynamoDB, the image ID is new, so an item is added.
- The benefits of processing in this way are as follows:
  - Even if duplicate images exist, only one copy is registered in the S3 bucket.
  - For example, `raw/abc.jpg/EF00EF.jpg` and `raw/bcd.jpg/EF00EF.jpg` are the same image, but the S3 bucket shows only one instance of EF00FF.jpg.
  - This improves storage efficiency.
  - Duplicate images are recorded as duplicates in the DynamoDB metadata, but the actual reference points to a single image, which is not an issue.

### Create a lambda deletion function
- Name: `ImageDeleteProcess`
- Code
```Python
# Combined a source code and a pseudo code
...

# Set DynamoDB table and S3 bucket through environment variables
DYNAMODB_TABLE_NAME = os.getenv('DYNAMODB_TABLE_NAME')
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')

def lambda_handler(event, context):
    try:
        # Extract the filename from the request
        # Query DynamoDB for the metadata of the given file
        # If no result is found, do not trigger an error; simply conclude as it does not exist.

        # Delete the file from the S3 bucket
        # Update the status to "deleted" in DynamoDB for all queried results
        
        # return 200 succcess code

    except Exception as e:
        # return 500 error code
```

### Modify configurations of the lambda function(`ImageDeleteProcess`)
- General configuration
  - Change the memory size: 1024MB
  - Change the ephemeral storage: 512MB
- Environment variables
  - `DYNAMODB_TABLE_NAME`: the table of DynamoDB(`image-metadata`)
  - `S3_BUCKET_NAME`: the image bucket of S3(`snail-images-dev`)
- Modify a role: `ImageDeleteProcess-role-xxxx`
  - Add policies: `AmazonDynamoDBBasicAccess`, `AmazonS3BasicAccess`
- If the configuration is comleted, click `Deploy`.